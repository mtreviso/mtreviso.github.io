<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<title>SLURM in the Wild ‚Äì Marcos Treviso</title>
<meta content="Complete guide to setting up SLURM job scheduler for academic research clusters. GPU scheduling, QoS policies, multi-node configuration, monitoring, and production optimizations." name="description"/>
<link href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'><rect width='100' height='100' fill='%230E1F1C'/><text x='50' y='65' font-size='52' text-anchor='middle' fill='%23E8D5B0' font-family='Georgia,serif' font-weight='bold'>MT</text></svg>" rel="icon"/>
<link href="https://fonts.googleapis.com" rel="preconnect"/>
<link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect"/>
<link href="https://fonts.googleapis.com/css2?family=Fraunces:ital,opsz,wght@0,9..144,300..900;1,9..144,300..700&family=Outfit:wght@300;400;500;600;700&display=swap" rel="stylesheet"/>
<link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" rel="stylesheet"/>
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" rel="stylesheet"/>
<script src="https://cdn.tailwindcss.com"></script>
<script>
  tailwind.config = {
    theme: {
      extend: {
        fontFamily: {
          display: ["Fraunces", "Georgia", "serif"],
          sans: ["Outfit", "ui-sans-serif", "system-ui", "sans-serif"],
        },
        colors: {
          ink:   "#0E1F1C",
          teal:  "#0A7C6A",
          parchment: "#F7F3EC",
          warm:  "#E8D5B0",
          muted: "#6B7C79",
          rule:  "#D5CFC3",
        }
      }
    }
  }
</script>
<style>
  :root {
    --ink:       #0E1F1C;
    --teal:      #0A7C6A;
    --parchment: #F7F3EC;
    --warm:      #E8D5B0;
    --muted:     #6B7C79;
    --rule:      #D5CFC3;
  }

  html { scroll-behavior: smooth; }
  @media (prefers-reduced-motion: reduce) { html { scroll-behavior: auto; } }

  body {
    background: var(--parchment);
    color: var(--ink);
    font-family: "Outfit", ui-sans-serif, system-ui, sans-serif;
  }

  /* ‚îÄ‚îÄ Grain texture ‚îÄ‚îÄ */
  body::before {
    content: "";
    position: fixed;
    inset: 0;
    background-image: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='300' height='300'%3E%3Cfilter id='n'%3E%3CfeTurbulence type='fractalNoise' baseFrequency='0.75' numOctaves='4' stitchTiles='stitch'/%3E%3C/filter%3E%3Crect width='300' height='300' filter='url(%23n)' opacity='0.04'/%3E%3C/svg%3E");
    pointer-events: none;
    z-index: 0;
    opacity: 0.5;
  }

  main, nav, footer { position: relative; z-index: 1; }

  /* ‚îÄ‚îÄ Typography ‚îÄ‚îÄ */
  h1, h2, h3 { font-family: "Fraunces", Georgia, serif; color: var(--ink); }
  h4 { font-family: "Outfit", sans-serif; font-weight: 600; color: var(--ink); }

  p { color: var(--ink); opacity: 0.85; }

  /* ‚îÄ‚îÄ Links ‚îÄ‚îÄ */
  a.prose-link, .narrative-section a:not(.no-style), .toc a {
    color: var(--teal);
    text-decoration: underline;
    text-underline-offset: 3px;
  }
  a.prose-link:hover, .narrative-section a:not(.no-style):hover, .toc a:hover {
    opacity: 0.75;
  }

  /* ‚îÄ‚îÄ Inline code ‚îÄ‚îÄ */
  code:not([class*="language-"]) {
    background: rgba(10,124,106,0.08);
    color: var(--ink);
    padding: 2px 6px;
    border-radius: 4px;
    font-size: 0.88em;
    font-family: 'SF Mono', 'Monaco', 'Consolas', monospace;
    border: 1px solid rgba(10,124,106,0.15);
  }

  /* ‚îÄ‚îÄ Code blocks ‚îÄ‚îÄ */
  pre[class*="language-"] {
    margin: 0 0 1.5em 0;
    border-radius: 0 0 10px 10px;
    box-shadow: 0 4px 20px rgba(14,31,28,0.15);
    font-size: 0.85em;
  }

  .code-block-header {
    display: flex;
    align-items: center;
    gap: 10px;
    background: #0f172a;
    color: #94a3b8;
    padding: 8px 16px;
    font-size: 0.8rem;
    font-weight: 500;
    border-radius: 10px 10px 0 0;
    font-family: 'SF Mono', 'Monaco', 'Consolas', monospace;
  }
  .code-block-header i { font-size: 0.9rem; opacity: 0.85; }
  .code-block-header a { color: #8b9d9a !important; }
  .code-block-header a:hover { color: #fff; }
  .code-block-header + pre[class*="language-"] { margin-top: 0 !important; border-radius: 0 0 10px 10px !important; }

  /* ‚îÄ‚îÄ Insight / Warning boxes ‚îÄ‚îÄ */
  .insight-box {
    background: rgba(10,124,106,0.06);
    border: 1px solid rgba(10,124,106,0.25);
    border-left: 3px solid var(--teal);
    border-radius: 10px;
    padding: 1.25rem 1.5rem;
    margin: 1.5rem 0;
  }
  .insight-box p { color: var(--ink); opacity: 1; }

  .warning-box {
    background: rgba(239,68,68,0.05);
    border: 1px solid rgba(239,68,68,0.2);
    border-left: 3px solid #ef4444;
    border-radius: 10px;
    padding: 1.25rem 1.5rem;
    margin: 1.5rem 0;
  }
  .warning-box p { color: #7f1d1d; opacity: 1; }

  /* ‚îÄ‚îÄ Section divider ‚îÄ‚îÄ */
  .section-divider {
    height: 1px;
    background: var(--rule);
    margin: 3rem 0;
  }

  /* ‚îÄ‚îÄ Table of Contents ‚îÄ‚îÄ */
  .toc {
    background: rgba(255,255,255,0.5);
    border: 1px solid var(--rule);
    border-radius: 12px;
    padding: 24px;
    margin: 24px 0;
  }
  .toc ul { list-style: none; padding-left: 0; }
  .toc li { margin: 7px 0; }
  .toc ul ul { padding-left: 20px; margin-top: 4px; }
  .toc ul ul li { margin: 3px 0; font-size: 0.875rem; }
  .toc a {
    color: var(--teal);
    text-decoration: none;
    padding: 2px 6px;
    border-radius: 4px;
    transition: background 0.15s;
  }
  .toc a:hover { background: rgba(10,124,106,0.1); }

  /* ‚îÄ‚îÄ Info cards ‚îÄ‚îÄ */
  .info-grid {
    display: grid;
    grid-template-columns: repeat(auto-fit, minmax(260px, 1fr));
    gap: 16px;
    margin: 24px 0;
  }
  .info-grid-three {
    display: grid;
    grid-template-columns: repeat(3, 1fr);
    gap: 16px;
    margin: 24px 0;
  }
  @media (max-width: 1024px) { .info-grid-three { grid-template-columns: 1fr; } }
  @media (min-width: 1025px) and (max-width: 1280px) { .info-grid-three { grid-template-columns: repeat(2, 1fr); } }

  .info-card {
    background: rgba(255,255,255,0.6);
    border: 1px solid var(--rule);
    border-radius: 12px;
    padding: 18px 20px;
    transition: border-color 0.2s, box-shadow 0.2s;
  }
  .info-card:hover {
    border-color: rgba(10,124,106,0.3);
    box-shadow: 0 4px 16px rgba(10,124,106,0.08);
  }
  .info-card p { opacity: 1; }

  /* ‚îÄ‚îÄ File callout ‚îÄ‚îÄ */
  .file-callout {
    background: rgba(255,255,255,0.5);
    border: 1px dashed var(--rule);
    padding: 14px 18px;
    border-radius: 10px;
  }

  /* ‚îÄ‚îÄ Narrative spacing ‚îÄ‚îÄ */
  .narrative-section { margin: 1.5rem 0; line-height: 1.75; }
  .narrative-section p { margin-bottom: 1rem; }

  /* ‚îÄ‚îÄ Server diagram ‚îÄ‚îÄ */
  .server-site {
    background: rgba(255,255,255,0.4);
    border: 1px solid var(--rule);
    border-radius: 14px;
    padding: 18px;
  }
  .server-node {
    background: rgba(255,255,255,0.7);
    border: 1px solid var(--rule);
    border-radius: 10px;
    padding: 14px;
    box-shadow: 0 1px 4px rgba(14,31,28,0.06);
  }

  /* ‚îÄ‚îÄ Hero image ‚îÄ‚îÄ */
  .hero-image { border-radius: 16px; }

  /* ‚îÄ‚îÄ Nav ‚îÄ‚îÄ */
  .nav-link { color: var(--muted); font-size: 0.875rem; transition: color 0.15s; }
  .nav-link:hover, .nav-link.active { color: var(--teal); }

  /* ‚îÄ‚îÄ Acknowledgments ‚îÄ‚îÄ */
  .ack-box {
    background: rgba(255,255,255,0.45);
    border: 1px solid var(--rule);
    border-radius: 12px;
    padding: 1.5rem;
  }

  /* ‚îÄ‚îÄ Breadcrumb ‚îÄ‚îÄ */
  .breadcrumb { color: var(--muted); font-size: 0.875rem; }
  .breadcrumb a { color: var(--muted); }
  .breadcrumb a:hover { color: var(--teal); }
</style>
</head>

<body>

<!-- ‚îÄ‚îÄ Navigation ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ -->
<nav class="sticky top-0 z-50 border-b border-rule bg-parchment/90 backdrop-blur-sm">
    <div class="max-w-4xl mx-auto px-6 py-3 flex justify-between items-center">
      <a href="../index.html" class="font-display font-semibold text-lg text-ink hover:text-teal transition-colors">Marcos Treviso</a>
      <div class="flex items-center gap-6 text-sm text-muted">
        <a href="../index.html#news" class="hover:text-ink transition-colors">News</a>
        <a href="../index.html#publications" class="hover:text-ink transition-colors">Publications</a>
        <a href="../index.htmlblog.html" class="text-ink font-medium">Blog</a>
        <a href="../CV.pdf" class="hover:text-ink transition-colors" target="_blank">CV</a>
      </div>
    </div>
  </nav>

<!-- Breadcrumb -->
<div class="max-w-4xl mx-auto px-4 sm:px-6 py-4">
  <nav class="breadcrumb">
    <a href="../index.html">Home</a>
    <span class="mx-2 text-rule">‚Ä∫</span>
    <a href="../blog.html">Blog</a>
    <span class="mx-2 text-rule">‚Ä∫</span>
    <span class="text-ink">SLURM Setup Guide</span>
  </nav>
</div>

<main class="max-w-4xl mx-auto px-4 sm:px-6 py-8">

  <!-- ‚îÄ‚îÄ Header ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ -->
  <header class="mb-12">
    <div class="flex justify-center mb-8">
      <div class="max-w-2xl w-full">
        <img alt="Cartoon illustration of eight anthropomorphic sardine characters representing Greek gods standing in front of glowing server racks in a colorful server room"
             class="w-full h-auto hero-image shadow-2xl"
             src="./figs/sardine-servers.jpeg"
             title="Eight Server Nodes Depicted as Greek Deities in the SARDINE Lab Data Center."/>
        <p class="text-sm text-center mt-3 italic" style="color:var(--muted)">The only gods we trust with our CUDA kernels.</p>
      </div>
    </div>

    <div class="text-sm mb-4" style="color:var(--muted)">Production Guide ¬∑ Updated on September 2025 ¬∑ 50 min read</div>
    <h1 class="font-display text-4xl sm:text-5xl font-semibold tracking-tight mb-5 leading-tight">
      SLURM in the Wild: A Practical Guide for Academic Labs
    </h1>
    <p class="text-lg leading-relaxed mb-8" style="color:var(--muted)">
      A complete guide from basic concepts to academic deployment, covering multi-node setup, GPU scheduling, advanced monitoring, and the hard-learned lessons from scaling a research lab from 2 to 30+ users across heterogeneous hardware.
    </p>
  </header>

  <!-- ‚îÄ‚îÄ Table of Contents ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ -->
  <section class="mb-16">
    <div class="toc">
      <h2 class="font-display text-xl font-semibold mb-4">Table of Contents</h2>
      <ul>
        <li><a href="#introduction">1. How We Improved Our Infrastructure</a></li>
        <li>
          <a href="#concepts">2. Understanding SLURM Core Concepts</a>
          <ul>
            <li><a href="#concepts">Cgroups: The Foundation of Resource Control</a></li>
            <li><a href="#concepts">Partitions: Organizing Your Hardware</a></li>
            <li><a href="#concepts">Quality of Service: The Art of Fair Scheduling</a></li>
            <li><a href="#concepts">The SLURM Ecosystem</a></li>
          </ul>
        </li>
        <li>
          <a href="#installation">3. Installation and Basic Setup</a>
          <ul>
            <li><a href="#installation">Prerequisites and Planning</a></li>
            <li><a href="#installation">Server Organization</a></li>
            <li><a href="#installation">Controller Node Installation</a></li>
            <li><a href="#installation">Compute Nodes Installation</a></li>
          </ul>
        </li>
        <li>
          <a href="#multi-node">4. Multi-Node Setup and Authentication</a>
          <ul>
            <li><a href="#multi-node">Munge: The Authentication Backbone</a></li>
            <li><a href="#multi-node">User and Group Synchronization</a></li>
            <li><a href="#multi-node">Network Configuration</a></li>
          </ul>
        </li>
        <li>
          <a href="#configuration">5. Configuration Files Deep Dive</a>
          <ul>
            <li><a href="#configuration">Database Configuration: slurmdbd.conf</a></li>
            <li><a href="#configuration">GPU Resource Mapping: gres.conf</a></li>
            <li><a href="#configuration">Primary Configuration: slurm.conf</a></li>
            <li><a href="#configuration">Node and Partition Definitions</a></li>
          </ul>
        </li>
        <li>
          <a href="#qos">6. Quality of Service Policies</a>
          <ul>
            <li><a href="#qos">Initialize the Accounting System</a></li>
            <li><a href="#qos">QoS Design Philosophy</a></li>
            <li><a href="#qos">Creating QoS Policies</a></li>
          </ul>
        </li>
        <li>
          <a href="#monitoring">7. Advanced Monitoring and Analytics</a>
          <ul>
            <li><a href="#monitoring">Enhanced Queue Viewer: psqueue</a></li>
            <li><a href="#monitoring">Installing Enhanced Tools</a></li>
          </ul>
        </li>
        <li>
          <a href="#usage">8. Real-World Usage Examples</a>
          <ul>
            <li><a href="#usage">Interactive Development Workflows</a></li>
            <li><a href="#usage">Production Batch Jobs</a></li>
          </ul>
        </li>
        <li>
          <a href="#troubleshooting">9. Troubleshooting and Maintenance</a>
          <ul>
            <li><a href="#troubleshooting">Node in DRAIN State</a></li>
            <li><a href="#troubleshooting">Jobs Stuck PENDING</a></li>
          </ul>
        </li>
        <li>
          <a href="#reporting">10. Interactive HTML Reports</a>
        </li>
        <li>
          <a href="#conclusion">11. Conclusion</a>
          <ul>
            <li><a href="#conclusion">What's next?</a></li>
          </ul>
        </li>
        <li><a href="#comments">12. Comments</a></li>
      </ul>
    </div>

    <p style="color:var(--muted); font-size:0.9rem;">
      Note: All configuration files and scripts mentioned in this guide are available in this
      <a href="https://github.com/mtreviso/slurm-setup/" rel="noopener" target="_blank">GitHub repository</a>.
    </p>
  </section>

  <!-- ‚îÄ‚îÄ 1. Introduction ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ -->
  <section class="mb-16" id="introduction">
    <h2 class="font-display text-3xl font-semibold mb-8">How We Improved Our Infrastructure</h2>
    <div class="narrative-section">
      <p>
        When I joined the <a href="https://sardine-lab.github.io/" rel="noopener" target="_blank">SARDINE Lab</a>
        in 2018‚Äì2019 (called <a href="http://deep-spin.github.io/" rel="noopener" target="_blank">DeepSPIN</a> at the time), our computing infrastructure was refreshingly simple: two machines with 4√ó GTX 1080 GPUs each, supporting a tight-knit group of 5 PhD students and 2 postdocs. For the NLP research we were doing at the time, this setup was more than adequate.
      </p>
      <p>
        Our resource allocation system was equally simple: a shared Google Spreadsheet where researchers would claim GPUs for their experiments. It worked well enough, except during those frenzied pre-deadline periods when everyone suddenly needed to run "large-scale" experiments simultaneously. The spreadsheet would become a battlefield of merged cells and conflicting claims, but we survived those chaotic moments through informal Slack negotiations and good-natured compromise.
      </p>
      <p>
        Fast-forward to today: our lab has grown to around 30 active researchers across 9 physically distributed servers featuring different GPU architectures‚Äîfrom older GTX cards to modern H100s and H200s. What started as manageable chaos had become completely unworkable. The spreadsheet method simply doesn't scale when you have dozens of users competing for resources across heterogeneous hardware. We were losing precious compute cycles to forgotten reservations, experiencing frequent conflicts over GPU access, and had no way to track actual resource utilization or ensure fair allocation.
      </p>
      <p>
        After evaluating various options, we decided to implement <a href="https://slurm.schedmd.com/documentation.html" rel="noopener" target="_blank">SLURM</a>. While initially an unpopular decision among some lab members who preferred the "freedom" of manual coordination, it has proven transformative. Now researchers submit jobs to intelligent queues that automatically allocate resources based on availability and priority. We have complete visibility into usage patterns, fair resource distribution, and the peace of mind that comes from professional job scheduling.
      </p>
      <p>
        However, I won't sugarcoat the journey‚Ä¶ Setting up SLURM is notoriously challenging. The documentation is dense, configuration files are numerous and interdependent, and examples for research lab environments (as opposed to traditional HPC centers) are scarce. Multi-node GPU clusters add another layer of complexity that can feel like navigating uncharted territory.
      </p>
      <p>
        This guide documents my real-world experience building a production SLURM cluster for academic research. Rather than jumping straight into configuration files, I'll start with the essential concepts that make SLURM tick. Understanding these fundamentals will make the subsequent setup much more intuitive and help you troubleshoot issues when they inevitably arise.
      </p>
    </div>
  </section>

  <div class="section-divider"></div>

  <!-- ‚îÄ‚îÄ 2. Core Concepts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ -->
  <section class="mb-16" id="concepts">
    <h2 class="font-display text-3xl font-semibold mb-8">Understanding SLURM Core Concepts</h2>
    <div class="narrative-section">
      <p class="text-lg">
        Before diving into installation, you need to understand SLURM's key concepts. Think of SLURM as an intelligent resource broker that sits between users and hardware, making decisions about who gets what resources and when. The magic happens through three core concepts that work together: cgroups for isolation, partitions for organization, and Quality of Service (QoS) for fairness.
      </p>

      <h3 class="font-display text-2xl font-semibold mb-5 mt-8">Cgroups: The Foundation of Resource Control</h3>
      <p>
        The first thing to understand is that SLURM doesn't just schedule jobs, it can also enforce resource limits. This is extremely useful, because without proper enforcement, a user requesting 1 GPU could accidentally (or intentionally) use all GPUs on a node, completely defeating the purpose of scheduling. This is where Linux control groups (cgroups) come in. Cgroups are Linux kernel features that isolate and limit resource usage for groups of processes. SLURM uses them to create "containers" around jobs, ensuring they can only access the CPU cores, memory, and devices they were allocated.
      </p>
      <p>
        Setting up cgroups requires both kernel configuration and SLURM configuration. First, we need to enable the right kernel parameters in <code>/etc/default/grub</code>:
      </p>

      <div class="code-block-header"><i class="fa-solid fa-file-code"></i>/etc/default/grub</div>
<pre><code class="language-bash"># Add cgroup options to kernel command line
GRUB_CMDLINE_LINUX="cgroup_enable=memory systemd.unified_cgroup_hierarchy=0"
</code></pre>

      After editing, update grub and reboot:

      <div class="code-block-header"><i class="fa-solid fa-terminal"></i></div>
<pre><code class="language-bash">sudo update-grub
sudo reboot
</code></pre>

      <p>
        Then configure SLURM to use cgroups for resource control by editing (or creating if it does not exist) the file <code>/etc/slurm/cgroup.conf</code>:
      </p>

      <div class="code-block-header"><i class="fa-solid fa-file-code"></i><a href="https://github.com/mtreviso/slurm-setup/blob/main/confs/cgroup.conf">/etc/slurm/cgroup.conf</a></div>
<pre><code class="language-ini">CgroupAutomount=yes
ConstrainCores=yes       # Limit CPU cores
ConstrainRAMSpace=yes    # Limit memory usage
ConstrainDevices=yes     # Control device access</code></pre>

      <p>And the specific devices that jobs are allowed to access in <code>/etc/slurm/cgroup_allowed_devices_file.conf</code>:</p>

      <div class="code-block-header"><i class="fa-solid fa-file-code"></i>/etc/slurm/cgroup_allowed_devices_file.conf</div>
<pre><code class="language-bash">/dev/null
/dev/urandom
/dev/zero
/dev/sda*
/dev/cpu/*/*
/dev/pts/*
/dev/nvidia*    # Allow GPU access</code></pre>

      <h3 class="font-display text-2xl font-semibold mb-5 mt-8">Partitions: Organizing Your Hardware</h3>
      <p>
        Once you have resource enforcement working, you need to organize your hardware logically. SLURM partitions are like job queues, but more powerful. They group nodes with similar characteristics and can have different policies, priorities, and access controls.
      </p>
      <p>
        In our lab, we organize partitions primarily by GPU type, since that's usually the limiting factor for our workloads. This allows researchers to request specific hardware for their experiments: <code>sbatch --partition=h100 my_large_model_job.sh</code> ensures the job runs on our high-memory H100 nodes, while <code>--partition=a6000</code> targets our more numerous A6000 nodes for less intensive training runs.
      </p>

      <h3 class="font-display text-2xl font-semibold mb-5 mt-4">Quality of Service: The Art of Fair Scheduling</h3>
      <p>
        Here's where SLURM gets really interesting. Quality of Service (QoS) policies are templates that define resource limits, priorities, and time constraints. But they're much more than simple quotas‚Äîthey're tools for shaping user behavior and encouraging efficient resource usage.
      </p>
      <p>
        The key insight is that good QoS design creates incentive alignment. Short jobs get high priority and generous resource limits, encouraging users to break large experiments into smaller pieces when possible. Long jobs get lower priority but extended time limits, ensuring important work can still complete. Emergency QoS levels provide escape hatches for urgent deadlines. We will talk more about Partitions and QoSs later.
      </p>

      <h3 class="font-display text-2xl font-semibold mb-5 mt-4">The SLURM Ecosystem</h3>
      <p>
        SLURM's architecture is elegantly simple yet powerful. At its core, you have <code>slurmd</code> daemons running on each compute node, communicating with a central <code>slurmctld</code> daemon on the management node. For a project like ours, you'll also run <code>slurmdbd</code> for accounting and historical data. The figure below illustrates how each daemon interacts with each other.
      </p>

      <center><img src="figs/slurm-arch.jpeg" class="max-w-md mb-4 rounded-xl shadow-lg"></center>

      <p>SLURM provides a comprehensive set of commands, but in practice, you'll use a core set regularly. Understanding these commands and their purposes will make the subsequent configuration much clearer:</p>

      <div class="info-grid-three">
        <div class="info-card">
          <h4 class="mb-3 flex items-center gap-2">
            <i class="fas fa-play" style="color:var(--teal)"></i> Job Submission
          </h4>
          <ul class="space-y-2 text-sm" style="color:var(--muted)">
            <li><strong class="text-ink">sbatch:</strong> Submit batch jobs</li>
            <li><strong class="text-ink">srun:</strong> Interactive job execution</li>
            <li><strong class="text-ink">salloc:</strong> Allocate resources</li>
            <li><strong class="text-ink">scancel:</strong> Cancel jobs</li>
          </ul>
        </div>
        <div class="info-card">
          <h4 class="mb-3 flex items-center gap-2">
            <i class="fas fa-chart-line" style="color:var(--teal)"></i> Monitoring
          </h4>
          <ul class="space-y-2 text-sm" style="color:var(--muted)">
            <li><strong class="text-ink">squeue:</strong> Job queue</li>
            <li><strong class="text-ink">sinfo:</strong> Partition information</li>
            <li><strong class="text-ink">sacct:</strong> Job usage history</li>
            <li><strong class="text-ink">scontrol:</strong> Administrative control</li>
          </ul>
        </div>
        <div class="info-card">
          <h4 class="mb-3 flex items-center gap-2">
            <i class="fas fa-users" style="color:var(--teal)"></i> Management
          </h4>
          <ul class="space-y-2 text-sm" style="color:var(--muted)">
            <li><strong class="text-ink">sacctmgr:</strong> Account management</li>
            <li><strong class="text-ink">sprio:</strong> Job priority analysis</li>
            <li><strong class="text-ink">sreport:</strong> Usage reports</li>
          </ul>
        </div>
      </div>

      <div class="insight-box">
        <p>
          Before installing SLURM, you may want to consider which plugins you will need for your installation. Refer to the list of possible plugins
          <a href="https://slurm.schedmd.com/quickstart_admin.html#build_install" rel="noopener" target="_blank">here</a>.
          In this guide, we will use two plugins: <code>cgroups</code> for resource enforcement and <code>munge</code> for authentication.
        </p>
      </div>
    </div>
  </section>

  <div class="section-divider"></div>

  <!-- ‚îÄ‚îÄ 3. Installation ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ -->
  <section class="mb-16" id="installation">
    <h2 class="font-display text-3xl font-semibold mb-8">Installation and Basic Setup</h2>
    <div class="narrative-section">
      <p class="text-lg">
        Now that you understand the concepts, let's build the actual system. My recommendation is to start simple: set up a single controller node that also runs compute jobs, get that working perfectly, then add additional compute nodes. This incremental approach makes debugging much easier. Plan for <strong>2‚Äì4 hours</strong> for initial setup and testing. SLURM has many interdependent components, and rushing through the installation often leads to hard-to-debug authentication and configuration issues.
      </p>

      <h3 class="font-display text-xl font-semibold mb-4 mt-8">Prerequisites and Planning</h3>
      <p>
        Before installing anything, ensure your environment meets the basic requirements. <strong>It's important that all nodes use the same Linux kernel and OS version.</strong> I recommend using Ubuntu with an LTS version (e.g., 22.04 LTS). At this point, we should have NVIDIA GPU drivers installed already. More critically, you need <strong>consistent user management</strong> across nodes. That is, all users should have the same UID and GID in <code>/etc/passwd</code>, including slurm-related accounts such as <code>munge</code> and <code>slurm</code>. Otherwise, authentication will fail in mysterious ways.
      </p>

      <h3 class="font-display text-xl font-semibold mb-4 mt-8">Server Organization</h3>
      <p>To make this guide more concrete, let's pretend we have a setup with 3 servers (with Ancient Greek God names, of course) in 2 different physical locations:</p>

      <div class="grid grid-cols-1 gap-5 md:grid-cols-2 my-6">
        <div class="server-site">
          <h3 class="text-sm font-semibold tracking-wide mb-3" style="color:var(--muted)">Location A</h3>
          <ul class="space-y-3">
            <li class="server-node">
              <div class="flex items-start gap-3">
                <div class="flex h-10 w-10 items-center justify-center rounded-lg text-lg" style="background:rgba(10,124,106,0.1)">üèπ</div>
                <div>
                  <div class="flex flex-wrap items-center gap-2">
                    <span class="font-semibold">artemis</span>
                    <span class="inline-flex items-center rounded-full px-2 py-0.5 text-[11px] font-semibold" style="background:var(--warm);color:var(--ink)">compute</span>
                    <span class="inline-flex items-center rounded-full px-2 py-0.5 text-[11px] font-semibold" style="background:rgba(10,124,106,0.15);color:var(--teal)">controller</span>
                  </div>
                  <p class="mt-1 text-sm" style="color:var(--muted)">8√ó A6000 (46 GB)</p>
                </div>
              </div>
            </li>
            <li class="server-node">
              <div class="flex items-start gap-3">
                <div class="flex h-10 w-10 items-center justify-center rounded-lg text-lg" style="background:rgba(10,124,106,0.1)">üçá</div>
                <div>
                  <div class="flex flex-wrap items-center gap-2">
                    <span class="font-semibold">dionysus</span>
                    <span class="inline-flex items-center rounded-full px-2 py-0.5 text-[11px] font-semibold" style="background:var(--warm);color:var(--ink)">compute</span>
                  </div>
                  <p class="mt-1 text-sm" style="color:var(--muted)">4√ó H100 (80 GB)</p>
                </div>
              </div>
            </li>
          </ul>
        </div>
        <div class="server-site">
          <h3 class="text-sm font-semibold tracking-wide mb-3" style="color:var(--muted)">Location B</h3>
          <ul class="space-y-3">
            <li class="server-node">
              <div class="flex items-start gap-3">
                <div class="flex h-10 w-10 items-center justify-center rounded-lg text-lg" style="background:rgba(239,68,68,0.08)">üî•</div>
                <div>
                  <div class="flex flex-wrap items-center gap-2">
                    <span class="font-semibold">hades</span>
                    <span class="inline-flex items-center rounded-full px-2 py-0.5 text-[11px] font-semibold" style="background:var(--warm);color:var(--ink)">compute</span>
                  </div>
                  <p class="mt-1 text-sm" style="color:var(--muted)">8√ó H200 (140 GB)</p>
                </div>
              </div>
            </li>
          </ul>
        </div>
      </div>

      <p>All servers are compute nodes since all of them have GPUs to run jobs. However, we need to select one of them to be a <strong>controller node</strong>. In our case, it's <strong>artemis</strong>.</p>

      <h3 class="font-display text-xl font-semibold mb-4 mt-8">Controller Node Installation</h3>
      <p>
        The controller node runs the central scheduling daemon (<code>slurmctld</code>), the accounting database daemon (<code>slurmdbd</code>), and typically a compute daemon (<code>slurmd</code>) if it also runs jobs. Start by installing all the necessary packages:
      </p>

      <div class="code-block-header"><i class="fa-solid fa-terminal"></i></div>
<pre><code class="language-bash"># Update system and install SLURM components
sudo apt update &amp;&amp; sudo apt upgrade -y
sudo apt install slurmd slurmctld slurm-client slurmdbd mariadb-server munge

# Install additional tools
sudo apt install mailutils  # For SLURM notifications
sudo systemctl enable slurmd slurmctld slurmdbd munge

# Additional packages
sudo apt install build-essential libpam0g-dev libmariadb-client-lgpl-dev libmysqlclient-dev mariadb-server libssl-dev
</code></pre>

      <p>Next, configure MariaDB for SLURM's accounting database. This database tracks every job, resource allocation, and usage metric‚Äîit's essential for QoS enforcement and reporting.</p>

      <div class="code-block-header"><i class="fa-solid fa-terminal"></i></div>
<pre><code class="language-sql">sudo systemctl enable mysql
sudo systemctl start mysql
sudo mysql -u root
</code></pre>

      Then, in the MySQL prompt:
      <div class="code-block-header"><i class="fa-solid fa-terminal"></i></div>
<pre><code class="language-sql">CREATE DATABASE slurm_acct_db;
CREATE USER 'slurm'@'localhost';
SET PASSWORD FOR 'slurm'@'localhost' = PASSWORD('slurmdbpass');
GRANT USAGE ON *.* TO 'slurm'@'localhost';
GRANT ALL PRIVILEGES ON slurm_acct_db.* TO 'slurm'@'localhost';
FLUSH PRIVILEGES;
EXIT;</code></pre>

      <p>Ideally you want to change the password to something different than "slurmdbpass". We will set the same password later in <code>/etc/slurm/slurmdbd.conf</code>.</p>

      <h4 class="text-lg font-semibold mb-2 mt-6">Database Performance Tuning</h4>
      <p>
        For busy clusters with hundreds of daily jobs, the default MariaDB configuration becomes a bottleneck. The accounting database handles constant writes as jobs start and finish, plus reads for priority calculations and reporting. Optimizing these settings can dramatically improve responsiveness:
      </p>

      <div class="code-block-header"><i class="fa-solid fa-file-code"></i>/etc/mysql/mariadb.conf.d/50-server.cnf</div>
<pre><code class="language-ini"># Optimizations for SLURM accounting database
innodb_buffer_pool_size=80G      # 50-80% of RAM
innodb_log_file_size=512M        # Larger for write-heavy workloads
innodb_lock_wait_timeout=900     # Longer timeouts for batch operations
</code></pre>

      <p>Database configuration changes require a restart and may need log file recreation:</p>

      <div class="code-block-header"><i class="fa-solid fa-terminal"></i></div>
<pre><code class="language-bash">sudo systemctl stop mariadb
sudo rm /var/lib/mysql/ib_logfile?  # Remove old log files
sudo systemctl start mariadb</code></pre>

      <h3 class="font-display text-xl font-semibold mb-4 mt-8">Compute Nodes Installation</h3>
      <p>Compute nodes are simpler: they only need the compute daemon and authentication. So, on each compute node, run:</p>

      <div class="code-block-header"><i class="fa-solid fa-terminal"></i></div>
<pre><code class="language-bash">sudo apt update
sudo apt install slurmd slurm-client munge
sudo systemctl enable slurmd munge</code></pre>
    </div>
  </section>

  <div class="section-divider"></div>

  <!-- ‚îÄ‚îÄ 4. Multi-Node ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ -->
  <section class="mb-16" id="multi-node">
    <h2 class="font-display text-3xl font-semibold mb-8">Multi-Node Setup and Authentication</h2>
    <div class="narrative-section">
      <p class="text-lg">
        Single-node SLURM is relatively straightforward, but multi-node deployments introduce authentication complexity that can be frustrating to debug. The key is understanding that SLURM components need to authenticate with each other constantly: the controller talks to compute nodes, nodes report back to the controller, and the database tracks everything.
      </p>

      <h3 class="font-display text-2xl font-semibold mb-5 mt-4">Munge: The Authentication Backbone</h3>
      <p>
        SLURM uses <a href="https://dun.github.io/munge" rel="noopener" target="_blank">Munge</a>, and so each message between SLURM daemons gets signed with a shared secret key, ensuring that only authorized processes can communicate.
      </p>
      <p>
        The setup process requires careful attention to file permissions and user synchronization. First, install and configure Munge on all nodes:
      </p>

      <div class="code-block-header"><i class="fa-solid fa-terminal"></i></div>
<pre><code class="language-bash"># Controller node
sudo apt-get install libmunge-dev libmunge2 munge -y
sudo systemctl enable munge
sudo systemctl start munge

# Compute nodes
sudo apt-get install libmunge-dev libmunge2 munge -y</code></pre>

      <p>The critical step is distributing the Munge key. This shared secret must be identical on all nodes:</p>

      <div class="code-block-header"><i class="fa-solid fa-terminal"></i></div>
<pre><code class="language-bash"># Copy key from controller to all compute nodes
sudo scp -p /etc/munge/munge.key username@compute-node:/etc/munge/munge.key

# Set proper permissions on all nodes (this is crucial!)
sudo chown -R munge: /etc/munge/ /var/log/munge/
sudo chmod 0700 /etc/munge/ /var/log/munge/</code></pre>

      <div class="warning-box">
        <p>
          <strong>‚ö†Ô∏è Warning:</strong> Incorrect file permissions are the most common cause of Munge authentication failures. The munge.key file must be readable only by the munge user, and the directories must have the exact permissions shown above.
        </p>
      </div>

      <p>For busy clusters, optimize Munge threading to handle the authentication load. For that, increase the number of threads in <code>/etc/default/munge</code>:</p>

      <div class="code-block-header"><i class="fa-solid fa-file-code"></i>/etc/default/munge</div>
<pre><code class="language-bash">OPTIONS="--num-threads 10"</code></pre>

      <p>Then, restart munge on all nodes:</p>

      <div class="code-block-header"><i class="fa-solid fa-terminal"></i></div>
<pre><code class="language-bash">sudo systemctl daemon-reload
sudo systemctl restart munge</code></pre>

      <p>Always test Munge authentication before proceeding:</p>

      <div class="code-block-header"><i class="fa-solid fa-terminal"></i></div>
<pre><code class="language-bash"># Test munge on each node
munge -n | unmunge
# Should show "STATUS: Success (0)"

# Test cross-node authentication
ssh compute-node "munge -n" | unmunge</code></pre>

      <h3 class="font-display text-2xl font-semibold mb-5 mt-8">User and Group Synchronization</h3>
      <p>
        Here's where many SLURM deployments fail: user and group IDs must be synchronized across all nodes. When the controller tells a compute node to run a job as user ID 1001, that ID must refer to the same user on both machines. More subtly, the <code>munge</code> and <code>slurm</code> system users must also have consistent IDs.
      </p>

      <div class="code-block-header"><i class="fa-solid fa-terminal"></i></div>
<pre><code class="language-bash"># Check current UIDs/GIDs on controller
sudo cat /etc/passwd | grep -P "slurm|munge"
# Example output:
# munge:x:64029:64029::/nonexistent:/usr/sbin/nologin
# slurm:x:64030:64030:,,,:/home/slurm:/bin/bash

# Synchronize on compute nodes (if needed)
sudo usermod -u 64029 munge
sudo groupmod -g 64029 munge
sudo usermod -u 64030 slurm
sudo groupmod -g 64030 slurm</code></pre>

      <h3 class="font-display text-2xl font-semibold mb-5 mt-8">Network Configuration</h3>
      <p>
        SLURM components communicate over specific TCP/UDP ports. In a trusted internal network, the simplest approach is to allow all traffic between cluster nodes:
      </p>

      <div class="code-block-header"><i class="fa-solid fa-terminal"></i></div>
<pre><code class="language-bash"># Open required ports on all nodes
sudo ufw allow 6817/tcp  # slurmctld
sudo ufw allow 6817/udp
sudo ufw allow 6818/tcp  # slurmd
sudo ufw allow 6818/udp
sudo ufw allow 6819/tcp  # slurmdbd
</code></pre>

      <p>Alternatively, you can allow all traffic between specific nodes:</p>

      <div class="code-block-header"><i class="fa-solid fa-terminal"></i></div>
<pre><code class="language-bash">sudo ufw allow from NODE_IP
</code></pre>
    </div>
  </section>

  <div class="section-divider"></div>

  <!-- ‚îÄ‚îÄ 5. Configuration ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ -->
  <section class="mb-16" id="configuration">
    <h2 class="font-display text-3xl font-semibold mb-8">Configuration Files Deep Dive</h2>

    <div class="file-callout text-sm mb-6" style="color:var(--muted)">
      <strong class="text-ink">Files to edit:</strong>
      <ul class="list-disc pl-6 mt-2 space-y-1">
        <li><code>/etc/slurm/cgroup.conf</code> (all nodes)</li>
        <li><code>/etc/slurm/cgroup_allowed_devices_file.conf</code> (all nodes)</li>
        <li><code>/etc/slurm/slurmdbd.conf</code> (controller only)</li>
        <li><code>/etc/slurm/gres.conf</code> (all nodes)</li>
        <li><code>/etc/slurm/slurm.conf</code> (all nodes)</li>
      </ul>
      <p class="mt-2">See repo: <a href="https://github.com/mtreviso/slurm-setup/" rel="noopener" target="_blank">github.com/mtreviso/slurm-setup</a></p>
    </div>

    <div class="narrative-section">
      <p class="text-lg">
        SLURM's behavior is controlled by several interconnected configuration files, and getting these right is crucial for a successful deployment. The main configuration file, <code>slurm.conf</code>, must be identical on all nodes (any mismatch will cause nodes to appear as "drained" and refuse to accept jobs). Before that, we will define how many GPUs we have available and from which type.
      </p>

      <h3 class="font-display text-2xl font-semibold mb-5 mt-8">Database Configuration: slurmdbd.conf</h3>
      <p>The database configuration is only needed on the controller node and requires careful attention to security:</p>

      <div class="code-block-header"><i class="fa-solid fa-file-code"></i><a href="https://github.com/mtreviso/slurm-setup/blob/main/confs/slurmdbd.conf">/etc/slurm/slurmdbd.conf</a></div>
<pre><code class="language-ini"># === DATABASE CONNECTION ===
AuthType=auth/munge
DbdHost=localhost
StorageHost=localhost
StorageLoc=slurm_acct_db
StoragePass=slurmdbpass
StorageType=accounting_storage/mysql
StorageUser=slurm
SlurmUser=slurm

# === LOGGING ===
LogFile=/var/log/slurm/slurmdbd.log
PidFile=/run/slurmdbd.pid</code></pre>

      <p>Afterwards, make sure the file has the correct permissions:</p>

      <div class="code-block-header"><i class="fa-solid fa-terminal"></i></div>
<pre><code class="language-bash">sudo chmod 600 /etc/slurm/slurmdbd.conf
sudo chown slurm:slurm /etc/slurm/slurmdbd.conf</code></pre>

      <h3 class="font-display text-2xl font-semibold mb-5 mt-8">GPU Resource Mapping: gres.conf</h3>
      <p>This file maps physical GPU devices to SLURM resources and must be created on all nodes:</p>

      <div class="code-block-header"><i class="fa-solid fa-file-code"></i><a href="https://github.com/mtreviso/slurm-setup/blob/main/confs/gres.conf">/etc/slurm/gres.conf</a></div>
<pre><code class="language-ini"># === GPU RESOURCE MAPPING ===
# Maps physical /dev/nvidia* devices to SLURM GPU resources

# Artemis node - A6000 GPUs
NodeName=artemis Type=a6000 Name=gpu File=/dev/nvidia0
NodeName=artemis Type=a6000 Name=gpu File=/dev/nvidia1
NodeName=artemis Type=a6000 Name=gpu File=/dev/nvidia2
NodeName=artemis Type=a6000 Name=gpu File=/dev/nvidia3
NodeName=artemis Type=a6000 Name=gpu File=/dev/nvidia4
NodeName=artemis Type=a6000 Name=gpu File=/dev/nvidia5
NodeName=artemis Type=a6000 Name=gpu File=/dev/nvidia6
NodeName=artemis Type=a6000 Name=gpu File=/dev/nvidia7

# Dionysus node - H100 GPUs
NodeName=dionysus Type=h100 Name=gpu File=/dev/nvidia0
NodeName=dionysus Type=h100 Name=gpu File=/dev/nvidia1
NodeName=dionysus Type=h100 Name=gpu File=/dev/nvidia2
NodeName=dionysus Type=h100 Name=gpu File=/dev/nvidia3

# Hades node - H200 GPUs
NodeName=hades Type=h200 Name=gpu File=/dev/nvidia0
NodeName=hades Type=h200 Name=gpu File=/dev/nvidia1
NodeName=hades Type=h200 Name=gpu File=/dev/nvidia2
NodeName=hades Type=h200 Name=gpu File=/dev/nvidia3
NodeName=hades Type=h200 Name=gpu File=/dev/nvidia4
NodeName=hades Type=h200 Name=gpu File=/dev/nvidia5
NodeName=hades Type=h200 Name=gpu File=/dev/nvidia6
NodeName=hades Type=h200 Name=gpu File=/dev/nvidia7</code></pre>

      <h3 class="font-display text-2xl font-semibold mb-5 mt-4">Primary Configuration: slurm.conf</h3>
      <p>Let's build the main configuration file section by section. This file defines your entire cluster topology, scheduling policies, and resource management settings:</p>

      <div class="code-block-header"><i class="fa-solid fa-file-code"></i><a href="https://github.com/mtreviso/slurm-setup/blob/main/confs/slurm.conf">/etc/slurm/slurm.conf</a></div>
<pre><code class="language-ini"># === CLUSTER IDENTIFICATION ===
ClusterName=sardine-cluster
SlurmctldHost=artemis  # Your controller hostname
MpiDefault=none

# === SLURM CONFIG ===
ReturnToService=1
SlurmctldPidFile=/run/slurmctld.pid
SlurmctldPort=6817
SlurmdPidFile=/run/slurmd.pid
SlurmdPort=6818
SlurmdSpoolDir=/var/lib/slurm/slurmd
SlurmUser=slurm
SlurmctldDebug=info
SlurmctldLogFile=/var/log/slurm/slurmctld.log
SlurmdDebug=debug2
SlurmdLogFile=/var/log/slurm/slurmd.log
StateSaveLocation=/var/lib/slurm/slurmctld
SwitchType=switch/none

# === TIMERS ===
InactiveLimit=0
KillWait=30
MinJobAge=300
SlurmctldTimeout=120
SlurmdTimeout=300
Waittime=0

# === RESOURCE MANAGEMENT ===
GresTypes=gpu                     # Enable GPU tracking
ProctrackType=proctrack/cgroup    # Use cgroups for process tracking
TaskPlugin=task/affinity,task/cgroup  # Enable cgroup

# === SCHEDULING ===
SchedulerType=sched/backfill        # Fill gaps with smaller jobs
SelectType=select/cons_tres         # Track individual resources
SelectTypeParameters=CR_CPU_Memory  # Consumable resources

# === JOB PRIORITY ===
PriorityType=priority/multifactor
PriorityWeightAge=10000           # Jobs gain priority over time
PriorityWeightQOS=250000          # QoS has high impact on priority

# === ACCOUNTING AND LIMITS ===
AccountingStorageEnforce=limits,qos
AccountingStorageType=accounting_storage/slurmdbd
AccountingStorageHost=artemis
AccountingStorageUser=slurm
AccountingStoreFlags=job_comment
AccountingStorageTRES=gres/gpu,gres/gpu:a6000,gres/gpu:h100,gres/gpu:h200

# === JOB OPTIONS ===
JobCompType=jobcomp/none
JobAcctGatherFrequency=30
JobAcctGatherType=jobacct_gather/none

# === COMPUTE NODES ===
NodeName=artemis  CPUs=112 Boards=1 SocketsPerBoard=2 CoresPerSocket=28 ThreadsPerCore=2 RealMemory=1031696 Gres=gpu:a6000:8
NodeName=dionysus CPUs=96  Boards=1 SocketsPerBoard=2 CoresPerSocket=24 ThreadsPerCore=2 RealMemory=1031564 Gres=gpu:h100:4
NodeName=hades    CPUs=192 Boards=1 SocketsPerBoard=2 CoresPerSocket=48 ThreadsPerCore=2 RealMemory=2063731 Gres=gpu:h200:8

# === PARTITIONS ===
PartitionName=a6000 Nodes=artemis  Default=NO MaxTime=INFINITE State=UP OverSubscribe=YES DefCpuPerGPU=8 DefMemPerCPU=12800 DefMemPerGPU=102400 AllowQos=cpu,gpu-debug,gpu-short,gpu-medium,gpu-long
PartitionName=h100  Nodes=dionysus Default=NO MaxTime=INFINITE State=UP OverSubscribe=YES DefCpuPerGPU=8 DefMemPerCPU=21550 DefMemPerGPU=172400 AllowQos=cpu,gpu-debug,gpu-short,gpu-h100
PartitionName=h200  Nodes=hades    Default=NO MaxTime=INFINITE State=UP OverSubscribe=YES DefCpuPerGPU=8 DefMemPerCPU=21550 DefMemPerGPU=172400 AllowQos=cpu,gpu-debug,gpu-short,gpu-h200
</code></pre>

      <p>There are many options in this file. I removed many commented options for the sake of clarity. Check out the original file to see all commented options: <a href="https://github.com/mtreviso/slurm-setup/blob/main/confs/slurm.conf" rel="noopener" target="_blank">github.com/mtreviso/slurm-setup</a>. Let's dive into some of the most important options.</p>

      <h3 class="font-display text-2xl font-semibold mb-5 mt-8">Node and Partition Definitions</h3>
      <p>
        Hardware specifications in SLURM must match reality <strong>exactly</strong>, or nodes will enter drained state. Use <code>sudo slurmd -C</code> on each node to get accurate specifications:
      </p>

      <div class="code-block-header"><i class="fa-solid fa-terminal"></i></div>
<pre><code class="language-bash"># On each node
sudo slurmd -C
</code></pre>

      <p>As output, you may obtain something like <code>NodeName=artemis CPUs=112 Boards=1 SocketsPerBoard=2 CoresPerSocket=28 ThreadsPerCore=2 RealMemory=1031696</code>.</p>
      <p>Copy the output and paste it as the node definition, then append the <code>Gres</code> information (which GPU types and how many ‚Äî these must match what you defined in <code>gres.conf</code>).</p>

<pre><code class="language-ini"># === NODE DEFINITIONS ===
NodeName=artemis  CPUs=112 Boards=1 SocketsPerBoard=2 CoresPerSocket=28 ThreadsPerCore=2 RealMemory=1031696 Gres=gpu:a6000:8
NodeName=dionysus CPUs=96  Boards=1 SocketsPerBoard=2 CoresPerSocket=24 ThreadsPerCore=2 RealMemory=1031564 Gres=gpu:h100:4
NodeName=hades    CPUs=192 Boards=1 SocketsPerBoard=2 CoresPerSocket=48 ThreadsPerCore=2 RealMemory=2063731 Gres=gpu:h200:8
</code></pre>

      <p>Once we have the node definitions, we can create our partitions. Three key parameters:</p>
      <ul class="list-disc ml-6 space-y-2 my-4" style="color:var(--muted)">
        <li><code>DefCpuPerGPU</code>: default number of CPUs per GPU ‚Äî choose a reasonable number that leaves a few cores for the OS.</li>
        <li><code>DefMemPerGPU</code>: default RAM per GPU ‚Äî similarly, leave some headroom for OS processes.</li>
        <li><code>AllowQos</code>: which QoS levels can be used in that partition. You can always edit this later and restart services.</li>
      </ul>

<pre><code class="language-ini"># === PARTITIONS ===
PartitionName=a6000 Nodes=artemis  Default=NO MaxTime=INFINITE \
    State=UP OverSubscribe=YES DefCpuPerGPU=8 DefMemPerGPU=102400 \
    AllowQos=cpu,gpu-debug,gpu-short,gpu-medium,gpu-long

PartitionName=h100  Nodes=dionysus Default=NO MaxTime=INFINITE \
    State=UP OverSubscribe=YES DefCpuPerGPU=8 DefMemPerGPU=172400 \
    AllowQos=cpu,gpu-debug,gpu-short,gpu-h100

PartitionName=h200  Nodes=hades    Default=NO MaxTime=INFINITE \
    State=UP OverSubscribe=YES DefCpuPerGPU=8 DefMemPerGPU=172400 \
    AllowQos=cpu,gpu-debug,gpu-short,gpu-h200
</code></pre>

      <p>So, <code>DefCpuPerGPU=8</code> automatically allocates 8 CPU cores for each GPU requested, while <code>DefMemPerGPU=102400</code> allocates about 100 GB of memory per GPU. <code>OverSubscribe=YES</code> allows more jobs than physical cores, useful for I/O-bound workloads.</p>
    </div>
  </section>

  <div class="section-divider"></div>

  <!-- ‚îÄ‚îÄ 6. QoS ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ -->
  <section class="mb-16" id="qos">
    <h2 class="font-display text-3xl font-semibold mb-8">Quality of Service Policies</h2>
    <div class="narrative-section">
      <p class="text-lg">
        QoS policies are where SLURM transforms from a simple job scheduler into an intelligent resource management system. I believe that QoS is something that needs to be discussed with the whole group and should not be set in stone. In our group, we consistently monitor SLURM usage and update our QoSs in order to maximize resource usage. The key insight: make the right thing to do also the easiest thing to do.
      </p>

      <h3 class="font-display text-2xl font-semibold mb-5 mt-4">Initialize the Accounting System</h3>
      <p>Before creating QoS policies, initialize the accounting database:</p>

      <div class="code-block-header"><i class="fa-solid fa-terminal"></i></div>
<pre><code class="language-bash"># Create cluster and account (run once on controller)
sudo sacctmgr add cluster sardine-cluster
sudo sacctmgr add account sardine Description="Research Account" Organization=university</code></pre>

      <p>Note that the cluster name needs to match the one defined in <code>/etc/slurm/slurm.conf</code>.</p>

      <h3 class="font-display text-2xl font-semibold mb-5 mt-8">QoS Design Philosophy</h3>
      <p>
        In our cluster, the QoS system creates a time-versus-priority trade-off. Short-running jobs get high priority and generous resource limits, encouraging users to break large experiments into smaller pieces. Long jobs get lower priority but extended time limits. Emergency QoS provides escape hatches for urgent deadlines. We also have specific QoSs granted on a per-user basis to control access to specific resources (e.g., H100s and H200s).
      </p>

      <div class="info-grid">
        <div class="info-card">
          <div class="flex items-center justify-between mb-3">
            <div class="flex items-center gap-3">
              <div class="rounded-lg p-2" style="background:rgba(239,68,68,0.1)"><i class="fas fa-bug" style="color:#dc2626"></i></div>
              <h4>gpu-debug</h4>
            </div>
            <span class="text-xs px-2 py-1 rounded-full font-semibold" style="background:rgba(239,68,68,0.1);color:#dc2626">Priority: 20</span>
          </div>
          <div class="text-sm space-y-2" style="color:var(--muted)">
            <p><strong class="text-ink">Purpose:</strong> Quick testing, debugging, interactive development</p>
            <p><strong class="text-ink">Limits:</strong> 1 job, up to 8 GPUs, 1 hour max</p>
            <p><strong class="text-ink">Philosophy:</strong> Highest priority for rapid iteration</p>
          </div>
        </div>

        <div class="info-card">
          <div class="flex items-center justify-between mb-3">
            <div class="flex items-center gap-3">
              <div class="rounded-lg p-2" style="background:rgba(234,88,12,0.1)"><i class="fas fa-fast-forward" style="color:#ea580c"></i></div>
              <h4>gpu-short</h4>
            </div>
            <span class="text-xs px-2 py-1 rounded-full font-semibold" style="background:rgba(234,88,12,0.1);color:#ea580c">Priority: 10</span>
          </div>
          <div class="text-sm space-y-2" style="color:var(--muted)">
            <p><strong class="text-ink">Purpose:</strong> Short experiments, hyperparameter sweeps, quick training</p>
            <p><strong class="text-ink">Limits:</strong> 2 jobs, up to 4 GPUs each, 4 hours max</p>
            <p><strong class="text-ink">Philosophy:</strong> High throughput for iterative research</p>
          </div>
        </div>

        <div class="info-card">
          <div class="flex items-center justify-between mb-3">
            <div class="flex items-center gap-3">
              <div class="rounded-lg p-2" style="background:rgba(37,99,235,0.1)"><i class="fas fa-clock" style="color:#2563eb"></i></div>
              <h4>gpu-medium</h4>
            </div>
            <span class="text-xs px-2 py-1 rounded-full font-semibold" style="background:rgba(37,99,235,0.1);color:#2563eb">Priority: 5</span>
          </div>
          <div class="text-sm space-y-2" style="color:var(--muted)">
            <p><strong class="text-ink">Purpose:</strong> Regular training runs, model development, evaluation</p>
            <p><strong class="text-ink">Limits:</strong> 1 job, up to 4 GPUs, 2 days max</p>
            <p><strong class="text-ink">Philosophy:</strong> Balanced resources for production work</p>
          </div>
        </div>

        <div class="info-card">
          <div class="flex items-center justify-between mb-3">
            <div class="flex items-center gap-3">
              <div class="rounded-lg p-2" style="background:rgba(107,33,168,0.1)"><i class="fas fa-calendar-alt" style="color:#7c3aed"></i></div>
              <h4>gpu-long</h4>
            </div>
            <span class="text-xs px-2 py-1 rounded-full font-semibold" style="background:rgba(107,33,168,0.1);color:#7c3aed">Priority: 2</span>
          </div>
          <div class="text-sm space-y-2" style="color:var(--muted)">
            <p><strong class="text-ink">Purpose:</strong> Extended training, large models, final experiments</p>
            <p><strong class="text-ink">Limits:</strong> 2 jobs, up to 2 GPUs each, 7 days max</p>
            <p><strong class="text-ink">Philosophy:</strong> Lower priority but extended time for big jobs</p>
          </div>
        </div>

        <div class="info-card">
          <div class="flex items-center justify-between mb-3">
            <div class="flex items-center gap-3">
              <div class="rounded-lg p-2" style="background:rgba(10,124,106,0.1)"><i class="fas fa-crown" style="color:var(--teal)"></i></div>
              <h4>gpu-h100</h4>
            </div>
            <span class="text-xs px-2 py-1 rounded-full font-semibold" style="background:rgba(10,124,106,0.1);color:var(--teal)">Priority: 10</span>
          </div>
          <div class="text-sm space-y-2" style="color:var(--muted)">
            <p><strong class="text-ink">Purpose:</strong> Only for people authorized to use H100s</p>
            <p><strong class="text-ink">Limits:</strong> 2 jobs, up to 4 GPUs each, unlimited time</p>
            <p><strong class="text-ink">Philosophy:</strong> Useful for large LLM training.</p>
          </div>
        </div>

        <div class="info-card">
          <div class="flex items-center justify-between mb-3">
            <div class="flex items-center gap-3">
              <div class="rounded-lg p-2" style="background:rgba(10,124,106,0.1)"><i class="fas fa-crown" style="color:var(--teal)"></i></div>
              <h4>gpu-h200</h4>
            </div>
            <span class="text-xs px-2 py-1 rounded-full font-semibold" style="background:rgba(10,124,106,0.1);color:var(--teal)">Priority: 10</span>
          </div>
          <div class="text-sm space-y-2" style="color:var(--muted)">
            <p><strong class="text-ink">Purpose:</strong> Only for people authorized to use H200s</p>
            <p><strong class="text-ink">Limits:</strong> 4 jobs, up to 4 GPUs each, unlimited time</p>
            <p><strong class="text-ink">Philosophy:</strong> Useful for even larger LLM training.</p>
          </div>
        </div>
      </div>

      <h3 class="font-display text-2xl font-semibold mb-5 mt-8">Creating QoS Policies</h3>
      <p>To add a QoS, use <code>sacctmgr</code>. Here is an example:</p>

      <div class="code-block-header"><i class="fa-solid fa-terminal"></i></div>
<pre><code class="language-bash"># Create QoS levels with carefully designed limits
sudo sacctmgr add qos cpu set priority=10 MaxJobsPerUser=4 \
    MaxTRESPerUser=cpu=32,mem=128G,gres/gpu=0

sudo sacctmgr add qos gpu-debug set priority=20 MaxJobsPerUser=1 \
    MaxTRESPerUser=gres/gpu=8 MaxWallDurationPerJob=01:00:00

sudo sacctmgr add qos gpu-short set priority=10 MaxJobsPerUser=2 \
    MaxTRESPerUser=gres/gpu=4 MaxWallDurationPerJob=04:00:00

sudo sacctmgr add qos gpu-medium set priority=5 MaxJobsPerUser=1 \
    MaxTRESPerUser=gres/gpu=4 MaxWallDurationPerJob=2-00:00:00

sudo sacctmgr add qos gpu-long set priority=2 MaxJobsPerUser=2 \
    MaxTRESPerUser=gres/gpu=2 MaxWallDurationPerJob=7-00:00:00

# Special QoS for H100/H200 nodes
sudo sacctmgr add qos gpu-h100 set priority=10 MaxJobsPerUser=2 \
    MaxTRESPerUser=gres/gpu=4 MaxWallDurationPerJob=2-00:00:00

sudo sacctmgr add qos gpu-h200 set priority=10 MaxJobsPerUser=4 \
    MaxTRESPerUser=gres/gpu=4 MaxWallDurationPerJob=4-00:00:00

# Emergency QoS for urgent situations / admins
sudo sacctmgr add qos gpu-hero set priority=100 MaxJobsPerUser=8 \
    MaxTRESPerUser=gres/gpu=8</code></pre>

      <p>
        Note that the math behind priority weighting matters. With <code>PriorityWeightQOS=250000</code> and <code>PriorityWeightAge=10000</code>, QoS dominates priority calculations. A job with QoS priority 100 gets 25,000,000 priority points, while age contributes at most a few thousand points per day. This ensures urgent jobs run immediately while still allowing aging for fairness. See <a href="https://slurm.schedmd.com/priority_multifactor.html#general" rel="noopener" target="_blank">SLURM's priority multifactor documentation</a> for the full math.
      </p>

      <p>Then, add users and grant them access to appropriate QoS levels:</p>

      <div class="code-block-header"><i class="fa-solid fa-terminal"></i></div>
<pre><code class="language-bash"># Add users and grant QoS access
sudo sacctmgr create user --immediate name=alice account=sardine \
    QOS=cpu,gpu-debug,gpu-short,gpu-medium,gpu-long

sudo sacctmgr create user --immediate name=bob account=sardine \
    QOS=cpu,gpu-debug,gpu-short,gpu-medium,gpu-long,gpu-h100

# Verify user configuration
sudo sacctmgr show user alice -s</code></pre>

      <p>
        The parameter breakdown: <strong>Priority</strong> determines run order (higher runs first), <strong>MaxJobsPerUser</strong> limits concurrent jobs, <strong>MaxTRESPerUser</strong> caps total resources, and <strong>MaxWallDurationPerJob</strong> sets time limits. Users choose appropriate QoS based on their job requirements, creating natural load balancing.
      </p>

      <p>Finally, restart services. Note that service startup order is critical for SLURM ‚Äî starting services in the wrong order may lead to authentication failures and jobs that refuse to start:</p>

      <div class="code-block-header"><i class="fa-solid fa-terminal"></i></div>
<pre><code class="language-bash"># Enable
sudo systemctl enable slurmdbd
sudo systemctl enable slurmctld
sudo systemctl enable slurmd

# Restart (in this order on the controller!)
sudo systemctl restart slurmdbd
sudo systemctl restart slurmctld
sudo systemctl restart slurmd

# Check status
sudo systemctl status slurmdbd
sudo systemctl status slurmctld
sudo systemctl status slurmd
</code></pre>

      <p>If something fails, check the logs in <code>/var/log/slurm/slurmdbd.log</code>, <code>/var/log/slurm/slurmctld.log</code>, and <code>/var/log/slurm/slurmd.log</code> for more information.</p>
    </div>
  </section>

  <div class="section-divider"></div>

  <!-- ‚îÄ‚îÄ 7. Monitoring ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ -->
  <section class="mb-16" id="monitoring">
    <h2 class="font-display text-3xl font-semibold mb-8">Advanced Monitoring and Analytics</h2>
    <div class="narrative-section">
      <p class="text-lg">
        Standard SLURM commands like <code>squeue</code> and <code>sinfo</code> are functional but provide a poor user experience. The output is hard to read, lacks crucial information like GPU allocations, and doesn't highlight relevant information for the current user. We can do much better.
      </p>

      <h3 class="font-display text-2xl font-semibold mb-5 mt-4">Enhanced Queue Viewer: psqueue</h3>
      <p>
        I've developed enhanced replacements that provide beautiful tabular output, GPU allocation details, memory usage information, and user highlighting. The difference is quite dramatic.
      </p>

      <p>Standard squeue shows basic information in a hard-to-read format:</p>

      <div class="code-block-header"><i class="fa-solid fa-terminal"></i>squeue</div>
<pre><code class="language-bash">JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
  123     a6000    train    alice  R       4:32      1 artemis
  124     a6000    eval     bob    PD      0:00      1 (Resources)
  125      h100    big    charlie  R  1-02:15:42    1 dionysus
</code></pre>

      <p>Pretty squeue (psqueue) provides beautiful tables with GPU and memory information:</p>

      <div class="code-block-header"><i class="fa-solid fa-terminal"></i>psqueue</div>
<pre><code class="language-bash">‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì
‚îÉ JOBID ‚îÉ NAME           ‚îÉ USER    ‚îÉ QOS        ‚îÉ START_TIME ‚îÉ TIME_LEFT  ‚îÉ CPUS ‚îÉ GPUS       ‚îÉ MEMORY      ‚îÉ STATE   ‚îÉ NODELIST            ‚îÉ
‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©
‚îÇ 71020 ‚îÇ python3        ‚îÇ dony    ‚îÇ gpu-medium ‚îÇ 1-19:18:32 ‚îÇ 2-00:00:00 ‚îÇ 100  ‚îÇ 4          ‚îÇ   0G (0%)   ‚îÇ PENDING ‚îÇ (Priority)          ‚îÇ
‚îÇ 71002 ‚îÇ cv-judge       ‚îÇ bob     ‚îÇ gpu-long   ‚îÇ -          ‚îÇ 11:03:13   ‚îÇ 8    ‚îÇ 1 (ID 3)   ‚îÇ 100G (100%) ‚îÇ RUNNING ‚îÇ artemis             ‚îÇ
‚îÇ 70916 ‚îÇ mt-explanation ‚îÇ miguel  ‚îÇ gpu-h100   ‚îÇ -          ‚îÇ 5-05:18:51 ‚îÇ 1    ‚îÇ 2 (ID 5-6) ‚îÇ 100G (50%)  ‚îÇ RUNNING ‚îÇ dionysus            ‚îÇ
‚îÇ 71101 ‚îÇ qwen-coder     ‚îÇ charlie ‚îÇ gpu-h200   ‚îÇ -          ‚îÇ 3-23:39:10 ‚îÇ 1    ‚îÇ 1 (ID 3)   ‚îÇ 168G (100%) ‚îÇ RUNNING ‚îÇ hades               ‚îÇ
‚îÇ 71076 ‚îÇ llama-pretrain ‚îÇ alice   ‚îÇ gpu-h200   ‚îÇ -          ‚îÇ 21:52:01   ‚îÇ 24   ‚îÇ 2 (ID 4-5) ‚îÇ 337G (100%) ‚îÇ RUNNING ‚îÇ hades               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
</code></pre>

      <p>Standard sinfo is very simple:</p>

      <div class="code-block-header"><i class="fa-solid fa-terminal"></i>sinfo</div>
<pre><code class="language-bash">PARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST
a6000        up   infinite      1    mix artemis
h100         up   infinite      1    mix dionysus
h200         up   infinite      1    mix hades
</code></pre>

      <p>Pretty sinfo (psinfo) provides beautiful tables and more information:</p>

      <div class="code-block-header"><i class="fa-solid fa-terminal"></i>psinfo</div>
<pre><code class="language-bash">‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì
‚îÉ NODE     ‚îÉ GPUS_USED        ‚îÉ GPUS    ‚îÉ MEM_USED   ‚îÉ MEMORY     ‚îÉ CPU_LOAD ‚îÉ CPUS ‚îÉ STATE ‚îÉ REASON         ‚îÉ
‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©
‚îÇ artemis  ‚îÇ 8 (ID 0-7)       ‚îÇ a6000:8 ‚îÇ 943.49 GB  ‚îÇ 1007.52 GB ‚îÇ 29.00%   ‚îÇ 112  ‚îÇ mixed ‚îÇ                ‚îÇ
‚îÇ dionysus ‚îÇ 3 (ID 0-1,3)     ‚îÇ h100:4  ‚îÇ 951.34 GB  ‚îÇ 1007.52 GB ‚îÇ 5.81%    ‚îÇ 112  ‚îÇ mixed ‚îÇ                ‚îÇ
‚îÇ hades    ‚îÇ 8 (ID 0-7)       ‚îÇ h200:8  ‚îÇ 1763.67 GB ‚îÇ 2015.36 GB ‚îÇ 4.82%    ‚îÇ 192  ‚îÇ mixed ‚îÇ                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
</code></pre>

      <h3 class="font-display text-2xl font-semibold mb-5 mt-8">Installing Enhanced Tools</h3>
      <p>
        The tools are standalone Python packages that require the <a href="https://github.com/Textualize/rich" rel="noopener" target="_blank">rich library</a>, installable per-user via <code>pip install --user rich</code>.
      </p>
      <p>You can find them in the GitHub repo:</p>

      <ul class="list-disc pl-6 mb-4" style="color:var(--muted)">
        <li><a href="https://github.com/mtreviso/slurm-setup/blob/main/scripts/psqueue.py" rel="noopener" target="_blank">psqueue.py</a></li>
        <li><a href="https://github.com/mtreviso/slurm-setup/blob/main/scripts/psinfo.py" rel="noopener" target="_blank">psinfo.py</a></li>
      </ul>

      <p>Installing is just a matter of copying the scripts to <code>/usr/local/bin</code>:</p>

      <div class="code-block-header"><i class="fa-solid fa-terminal"></i></div>
<pre><code class="language-bash">sudo cp psqueue.py /usr/local/bin/psqueue
sudo chmod +x /usr/local/bin/psqueue
sudo cp psinfo.py /usr/local/bin/psinfo
sudo chmod +x /usr/local/bin/psinfo
</code></pre>

      <p>The enhanced tools support <strong>all the same arguments</strong> as their SLURM counterparts:</p>

      <div class="code-block-header"><i class="fa-solid fa-terminal"></i></div>
<pre><code class="language-bash"># Enhanced queue display
psqueue

# Show only your jobs
psqueue --user=$USER

# Show only pending jobs with reasons
psqueue --states=PENDING

# Enhanced node information
psinfo

# Force ASCII output for scripts
psqueue --plain
psinfo --plain</code></pre>
    </div>
  </section>

  <div class="section-divider"></div>

  <!-- ‚îÄ‚îÄ 8. Usage ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ -->
  <section class="mb-16" id="usage">
    <h2 class="font-display text-3xl font-semibold mb-8">Real-World Usage Examples</h2>
    <div class="narrative-section">
      <p class="text-lg">
        Here are some common examples that researchers actually use, from quick debugging sessions to large-scale training runs.
      </p>

      <h3 class="font-display text-xl font-semibold mb-5 mt-4">Interactive Development Workflows</h3>
      <p>
        Interactive sessions are crucial for research work ‚Äî debugging code, testing models, and exploring datasets. The key is making these sessions fast to obtain (high priority) but limited in scope to prevent abuse. To launch an interactive session, pass <code>--pty bash</code> to <code>srun</code>:
      </p>

      <div class="info-card mb-6">
        <h4 class="mb-3">Examples with <code>srun</code></h4>
        <div class="code-block-header"><i class="fa-solid fa-terminal"></i></div>
<pre class="text-xs mb-0"><code class="language-bash"># Immediate access to 1 GPU for testing in artemis.
srun -p a6000 -w artemis --gres=gpu:1 --qos=gpu-debug --pty bash

# 4 hours with 4 A6000 GPUs in artemis.
srun -p a6000 -w artemis --gres=gpu:4 --qos=gpu-short --time=04:00:00 --pty bash

# Request specific H100 GPUs in the correct node (dionysus).
srun -p h100 -w dionysus --gres=gpu:h100:2 --qos=gpu-h100 --pty bash
</code></pre>
      </div>

      <h3 class="font-display text-xl font-semibold mb-5 mt-8">Production Batch Jobs</h3>
      <p>
        Batch jobs are where SLURM really shines. A well-written job script includes proper resource requests, environment setup, logging, and error handling. Here's a complete example:
      </p>

      <div class="code-block-header"><i class="fa-solid fa-file-code"></i><a href="https://github.com/mtreviso/slurm-setup/blob/main/examples/training-job.sh">examples/training-job.sh</a></div>
<pre><code class="language-bash">#!/bin/bash
# Complete SLURM batch script example

# === SLURM JOB PARAMETERS ===
# Slurm parameters are informed via the #SBATCH directive (yes, like a comment)

#SBATCH --job-name=bert-large-training
#SBATCH --gres=gpu:a6000:4              # 4 A6000 GPUs
#SBATCH --qos=gpu-medium                # Medium priority queue
#SBATCH --time=1-12:00:00               # 36 hours
#SBATCH --partition=a6000               # A6000 partition
#SBATCH --cpus-per-task=32              # 8 CPUs per GPU
#SBATCH --mem=100G                      # 25GB per GPU
#SBATCH --output=logs/training-%j.out   # %j = job ID
#SBATCH --error=logs/training-%j.err

# === ENVIRONMENT SETUP ===
# module load python/3.11.4 cuda/12.1.0
source /mnt/scratch/alice/envs/training/bin/activate

# === JOB INFO LOGGING ===
echo "Job started at: $(date)"
echo "Job ID: $SLURM_JOB_ID"
echo "Node: $SLURM_NODELIST"
echo "GPUs: $CUDA_VISIBLE_DEVICES"
echo "Working directory: $(pwd)"

# === ACTUAL TRAINING ===
cd /mnt/data/alice/bert-project

python -m torch.distributed.launch \
    --nproc_per_node=$SLURM_GPUS_ON_NODE \
    --nnodes=$SLURM_NNODES \
    --node_rank=$SLURM_PROCID \
    --master_addr=$SLURM_LAUNCH_NODE_IPADDR \
    --master_port=29500 \
    train.py \
        --config configs/bert-large.yaml \
        --output_dir checkpoints/bert-large-$(date +%Y%m%d) \
        --logging_dir logs/tensorboard-$SLURM_JOB_ID

echo "Job finished at: $(date)"
</code></pre>

      <p>All we have to do is submit the job using <code>sbatch</code>:</p>

      <div class="code-block-header"><i class="fa-solid fa-terminal"></i></div>
<pre><code class="language-bash">sbatch training-job.sh
</code></pre>

      <p>Your job will be given an ID by SLURM (e.g., <code>12345</code>). At this point, you can monitor all jobs, including yours, using <code>psqueue</code>. The output of your job (stdout) will be saved in <code>logs/training-12345.out</code>.</p>
    </div>
  </section>

  <div class="section-divider"></div>

  <!-- ‚îÄ‚îÄ 9. Troubleshooting ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ -->
  <section class="mb-16" id="troubleshooting">
    <h2 class="font-display text-3xl font-semibold mb-8">Troubleshooting and Maintenance</h2>
    <div class="narrative-section">
      <p class="text-lg">Most problems fall into a few categories: hardware specification mismatches, authentication failures, resource conflicts, and performance bottlenecks.</p>

      <h3 class="font-display text-2xl font-semibold mb-5 mt-4">Node in DRAIN State</h3>
      <p>
        This is by far the most common issue. When this happens, nodes appear as "drain", "drng", or "down" in <code>psinfo</code> output. This almost always indicates a mismatch between the hardware specifications in <code>slurm.conf</code> and the actual hardware SLURM detects on the node.
      </p>
      <p>
        I recommend checking <code>/etc/slurm/slurm.conf</code> and making sure all Node values are correct, according to what you obtain with <code>free -m</code> and <code>sudo slurmd -C</code>.
      </p>
      <p>The simplest solution is to try the following command, which sets a specific node back to the RESUME state:</p>

      <div class="code-block-header"><i class="fa-solid fa-terminal"></i></div>
<pre><code class="language-bash">sudo scontrol update NodeName=nodename State=RESUME
</code></pre>

      <p>If that doesn't work, check logs via <code>sudo journalctl -u slurmd --since "1 hour ago"</code> or directly in <code>/var/log/slurm/*</code>.</p>

      <h3 class="font-display text-2xl font-semibold mb-5 mt-8">Jobs Stuck PENDING</h3>
      <p>The enhanced queue viewer makes diagnosing pending jobs much easier by showing detailed reasons:</p>

      <div class="code-block-header"><i class="fa-solid fa-terminal"></i></div>
<pre><code class="language-bash"># See detailed pending reasons
psqueue --states=PENDING</code></pre>

      <p>Common pending reasons and their meanings:</p>
      <ul class="list-disc list-inside space-y-2 mt-3" style="color:var(--muted)">
        <li><strong class="text-ink">Priority:</strong> Higher priority jobs waiting ‚Üí normal, will run eventually</li>
        <li><strong class="text-ink">Resources:</strong> Not enough free GPUs/memory ‚Üí wait or reduce request</li>
        <li><strong class="text-ink">QOSMaxGRESPerUser:</strong> User exceeded GPU limit ‚Üí wait for jobs to finish</li>
        <li><strong class="text-ink">BadConstraints:</strong> Invalid resource request ‚Üí fix job parameters</li>
        <li><strong class="text-ink">PartitionNodeLimit:</strong> Partition full ‚Üí try different partition</li>
      </ul>
    </div>
  </section>

  <div class="section-divider"></div>

  <!-- ‚îÄ‚îÄ 10. Reporting ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ -->
  <section class="mb-16" id="reporting">
    <h2 class="font-display text-3xl font-semibold mb-8">Interactive HTML Reports</h2>
    <div class="narrative-section">
      <p class="text-lg">
        Production clusters generate vast amounts of usage data that can provide insights into user behavior, resource efficiency, and policy effectiveness. Automated reporting transforms this raw data into actionable insights for capacity planning and optimization.
      </p>
      <p>
        How to decide the characteristics of each QoS? How to know if the server is idle and too many jobs are just stuck in the queue? SLURM provides a vast amount of usage data via <code>sacct</code>, but all that data comes in a format that is almost impossible to read. Therefore, I decided to create a script that transforms that raw data into actionable insights.
      </p>
      <p>
        The <a href="https://github.com/mtreviso/cluster-scope" rel="noopener" target="_blank">cluster-scope</a> script generates comprehensive HTML reports with interactive charts and analytics. These reports help identify usage patterns, efficiency metrics, and optimization opportunities. Report features include interactive visualizations (job state distribution, timeline analysis), resource utilization by user/QoS, queue performance metrics, efficiency rates, and capacity planning recommendations based on actual usage patterns.
      </p>

      <div class="code-block-header"><i class="fa-solid fa-terminal"></i></div>
<pre><code class="language-bash"># Install dependencies
pip install pandas matplotlib numpy seaborn jinja2

# Generate comprehensive report
sudo python3 slurm_report.py --start-date 2025-01-01
</code></pre>

      <p>That's all!</p>
    </div>
  </section>

  <div class="section-divider"></div>

  <!-- ‚îÄ‚îÄ 11. Conclusion ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ -->
  <section class="mb-16" id="conclusion">
    <h2 class="font-display text-3xl font-semibold mb-8">Conclusion</h2>
    <div class="narrative-section">
      <p class="text-lg">
        This guide has taken you from basic concepts to a fully operational, production-ready SLURM cluster with advanced monitoring, analytics, and optimization features. What started as a solution to our lab's spreadsheet chaos has become a robust system that fairly allocates resources, encourages efficient usage patterns, and provides valuable insights into research computing patterns.
      </p>
      <p class="text-lg">
        You now have a production-ready SLURM cluster with sophisticated QoS policies, beautiful monitoring tools, automated reporting, and optimization features that rival commercial HPC installations üöÄ
      </p>

      <h3 class="font-display text-2xl font-semibold mb-5 mt-8">What's next?</h3>
      <p class="text-lg">
        A cluster is only as good as its management. Regular monitoring, user feedback, and continuous optimization will ensure your SLURM deployment remains effective and valuable for your research community. I strongly believe that the time invested in proper setup pays off in research productivity, fair resource access, and reduced administrative overhead (trust me, life is so much better with SLURM).
      </p>
      <p class="text-lg">In that spirit, there are many more things you'll need to set up for a seamless experience:</p>

      <ul class="list-disc pl-6 mt-4 space-y-3" style="color:var(--muted)">
        <li>
          <strong class="text-ink">Shared filesystem:</strong> In our clusters, we use <a href="https://docs.gluster.org/en/latest/" rel="noopener" target="_blank">GlusterFS</a> as the shared filesystem for <code>home</code> directories, so that all users have a unique home folder regardless of which server they log in to.
        </li>
        <li>
          <strong class="text-ink">NFS mountpoints:</strong> I strongly suggest dividing disks into three categories: <code>home</code> (small disks, RAID 1) for code and scripts, <code>data</code> (large disks, RAID 5/6) for important large files, and <code>scratch</code> (large disks, RAID 0) for datasets and model checkpoints. We use NFS for <code>data</code> and <code>scratch</code> so they're accessible from all servers.
        </li>
        <li>
          <strong class="text-ink">Quota:</strong> Without quota, people will download data and generate checkpoints up to the disk limit. A quota system helps maintain fair use of disk space.
        </li>
        <li>
          <strong class="text-ink">Spack and LMOD:</strong> Having the option to start a project with the correct version of Python, CUDA, or other libraries is very important. The combination of Spack and LMOD is great for this ‚Äî just do <code>module load python/3.13</code> and go.
        </li>
      </ul>

      <p class="text-lg mt-6">
        All configuration files, scripts, and tools mentioned in this guide are available in the
        <a href="https://github.com/mtreviso/slurm-setup/" rel="noopener" target="_blank">accompanying GitHub repository</a>.
        I have also created the following additional resources:
      </p>

      <ul class="list-disc pl-6 mt-4 space-y-2" style="color:var(--muted)">
        <li>
          <strong class="text-ink">For users:</strong>
          <a href="https://gist.github.com/mtreviso/ee2517f69211af8f27feef9b11cddab4" target="_blank" rel="noopener">Quickstart guide for launching &amp; managing jobs</a>
        </li>
        <li>
          <strong class="text-ink">For admins:</strong>
          <a href="https://gist.github.com/mtreviso/051761a0ae51a2aad220e158667361ba" target="_blank" rel="noopener">Cluster management &amp; QoS setup notes</a>
        </li>
      </ul>

      <p class="text-lg mt-6">Both are living docs. Feel free to send PRs or comments if you have improvements!</p>
    </div>
  </section>

  <!-- Acknowledgments -->
  <section class="mb-12">
    <div class="ack-box text-center">
      <p style="color:var(--muted)">
        <strong class="text-ink">Acknowledgments:</strong> Special thanks to the true SARDINE warriors, Duarte Alves and Sweta Agrawal, whose patience, expertise, and funny debugging sessions made this SLURM guide possible üôè
      </p>
    </div>
  </section>

  <!-- ‚îÄ‚îÄ Comments ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ -->
  <div class="section-divider"></div>
  <section id="comments" class="mt-10 mb-16">
    <h2 class="font-display text-2xl font-semibold mb-6">Comments</h2>
    <div class="rounded-xl border p-4" style="border-color:var(--rule);background:rgba(255,255,255,0.4)">
      <script src="https://giscus.app/client.js"
          data-repo="mtreviso/slurm-setup"
          data-category="General"
          data-mapping="pathname"
          data-strict="0"
          data-reactions-enabled="1"
          data-emit-metadata="0"
          data-input-position="bottom"
          data-theme="light"
          data-lang="en"
          crossorigin="anonymous"
          async>
      </script>
      <noscript class="text-sm" style="color:var(--muted)">Enable JavaScript to view comments powered by Giscus.</noscript>
    </div>
  </section>

</main>

<!-- ‚îÄ‚îÄ Footer ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ -->
<footer class="border-t py-10 mt-4" style="border-color:var(--rule)">
  <div class="max-w-5xl mx-auto px-4 sm:px-6 flex flex-col sm:flex-row justify-between items-center gap-4">
    <p class="text-sm" style="color:var(--muted)">
      ¬© <span id="year"></span> Marcos Treviso ¬∑ Lisbon, Portugal
    </p>
    <div class="flex gap-4">
      <a href="https://scholar.google.com/citations?user=puR_FskAAAAJ" target="_blank" aria-label="Google Scholar" class="text-xl transition-colors hover:text-teal" style="color:var(--muted)"><i class="fas fa-graduation-cap"></i></a>
      <a href="https://github.com/mtreviso" target="_blank" aria-label="GitHub" class="text-xl transition-colors hover:text-teal" style="color:var(--muted)"><i class="fab fa-github"></i></a>
      <a href="https://twitter.com/marcos_treviso" target="_blank" aria-label="Twitter" class="text-xl transition-colors hover:text-teal" style="color:var(--muted)"><i class="fab fa-twitter"></i></a>
      <a href="https://linkedin.com/in/marcostreviso" target="_blank" aria-label="LinkedIn" class="text-xl transition-colors hover:text-teal" style="color:var(--muted)"><i class="fab fa-linkedin"></i></a>
    </div>
  </div>
</footer>

<!-- Prism.js -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>
<script>document.getElementById("year").textContent = new Date().getFullYear();</script>

</body>
</html>